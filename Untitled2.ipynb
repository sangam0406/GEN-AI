{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import torch\n",
        "\n",
        "csv_file_path = \"/content/cleaned_first_100_issues.csv\"\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "\n",
        "indexes_to_include = [0, 1, 2, 3, 4]\n",
        "df_filtered = df.iloc[indexes_to_include]\n",
        "\n",
        "\n",
        "dataset = Dataset.from_pandas(df_filtered)\n",
        "\n",
        "\n",
        "base_model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "refined_model = \"llama-2-7b-mlabonne-enhanced\"\n",
        "\n",
        "llama_tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
        "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
        "llama_tokenizer.padding_side = \"right\"  # Fix for fp16\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=False\n",
        ")\n",
        "\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    quantization_config=quant_config,\n",
        "    device_map={\"\": 0}\n",
        ")\n",
        "base_model.config.use_cache = False\n",
        "base_model.config.pretraining_tp = 1\n",
        "\n",
        "max_length = 512\n",
        "\n",
        "\n",
        "def calculate_weights(text):\n",
        "\n",
        "\n",
        "    tokens = llama_tokenizer.tokenize(text)\n",
        "    weights = [1] * len(tokens)\n",
        "    return tokens, weights\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    texts = examples[\"body\"]\n",
        "    tokenized_inputs = {\"input_ids\": [], \"attention_mask\": []}\n",
        "\n",
        "    for text in texts:\n",
        "        tokens, weights = calculate_weights(text)\n",
        "        weighted_tokens = [token for token, weight in zip(tokens, weights) for _ in range(weight)]\n",
        "        encoded = llama_tokenizer(\" \".join(weighted_tokens), padding=\"max_length\", truncation=True, max_length=max_length)\n",
        "        tokenized_inputs[\"input_ids\"].append(encoded[\"input_ids\"])\n",
        "        tokenized_inputs[\"attention_mask\"].append(encoded[\"attention_mask\"])\n",
        "\n",
        "    return tokenized_inputs\n",
        "\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n",
        "\n",
        "\n",
        "peft_parameters = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=8,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "\n",
        "peft_model = get_peft_model(base_model, peft_parameters)\n",
        "\n",
        "\n",
        "train_params = TrainingArguments(\n",
        "    output_dir=\"./results_modified\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=1,\n",
        "    optim=\"adamw_hf\",\n",
        "    save_steps=25,\n",
        "    logging_steps=25,\n",
        "    learning_rate=2e-1,\n",
        "    weight_decay=0.001,\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=0.03,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"constant\"\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    train_dataset=tokenized_datasets,\n",
        "    tokenizer=llama_tokenizer,\n",
        "    args=train_params\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "trainer.save_model(refined_model)\n"
      ],
      "metadata": {
        "id": "QLnWw8oANkQK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}